import pandas as pd
import numpy as np
import joblib
from datetime import datetime
import warnings

# Scikit-learn and XGBoost imports
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import xgboost as xgb

# Pybaseball for data fetching
try:
    from pybaseball import pitching_stats_range, team_fielding_bref, team_batting_bref
    from pybaseball.cache import enable
    enable()
    print("Pybaseball imported and cache enabled successfully!")
except ImportError:
    print("Pybaseball not installed. Please install it using: pip install pybaseball")
    exit()

# Suppress irrelevant warnings for cleaner output
warnings.filterwarnings('ignore')

# ==============================================================================
# 1. Configuration Class
# ==============================================================================
class Config:
    """Centralized configuration for the advanced training script."""
    TRAIN_START_YEAR = 2021
    TRAIN_END_YEAR = 2023
    TEST_YEAR = 2024
    
    MIN_INNINGS_PITCHED = 40
    MIN_STARTS_FOR_STARTER = 5 # Min games started to be considered a "starter"

    MODEL_PATH = 'mlb_advanced_xgb_model.joblib'
    SCALER_PATH = 'mlb_advanced_scaler.joblib'
    
    # --- New, more advanced features ---
    FEATURE_COLUMNS = [
        'starter_siera_diff', 'starter_whip_diff',
        'bullpen_siera_diff', 'bullpen_whip_diff',
        'ops_plus_diff', 'drs_diff'
    ]
    TARGET_COLUMN = 'home_team_won'

    # Time-decay parameter: smaller value = slower decay (older games matter more)
    TIME_DECAY_RATE = 0.002

config = Config()

# ==============================================================================
# 2. Data Fetching Module (Upgraded)
# ==============================================================================
def fetch_advanced_historical_data(start_year, end_year):
    """Fetches advanced pitching (starter/bullpen), fielding, and batting data."""
    print(f"Fetching data from {start_year} to {end_year}...")
    
    # 1. Fetch Pitcher Stats and Separate Starters/Bullpen
    print("Fetching and separating pitcher stats...")
    pitcher_df = pitching_stats_range(f"{start_year}-01-01", f"{end_year}-12-31")
    pitcher_df = pitcher_df[pitcher_df['IP'] >= config.MIN_INNINGS_PITCHED]
    # Use different column names for FanGraphs data to avoid clashes
    pitcher_df.rename(columns={'Team': 'FG_Team'}, inplace=True)

    starters_df = pitcher_df[pitcher_df['GS'] >= config.MIN_STARTS_FOR_STARTER].copy()
    bullpen_df = pitcher_df[pitcher_df['GS'] < config.MIN_STARTS_FOR_STARTER].copy()

    # 2. Fetch Team Fielding Stats
    print("Fetching team fielding stats...")
    fielding_df = team_fielding_bref(start_season=start_year, end_season=end_year)
    fielding_df = fielding_df[['Season', 'Tm', 'DefEff']] # Using Defensive Efficiency

    # 3. Fetch Team Batting Stats
    print("Fetching team batting stats...")
    batting_df = team_batting_bref(start_season=start_year, end_season=end_year)
    # OPS+ is a great all-in-one offensive metric, adjusted for park/league
    batting_df = batting_df[['Season', 'Tm', 'OPS+']] 
    
    # 4. Fetch Game Schedules and Results (from Baseball-Reference)
    print("Fetching game schedules and results...")
    # Baseball-Reference team abbreviations are more consistent
    bref_teams = batting_df['Tm'].unique()
    all_games = []
    
    for year in range(start_year, end_year + 1):
        for team in bref_teams:
            # We need a robust way to get full schedules. pybaseball's schedule_and_record is good for this.
            # We assume a function get_schedule(year) exists. For this example, we'll simulate it.
            # In a real scenario, you'd integrate pybaseball.schedule_and_record and clean the team names.
            # This part is complex, so we'll construct a simplified version from the batting data.
            pass # Placeholder for robust schedule fetching. We'll build features on a team-season level.
    
    # For this example, we will simulate game matchups by creating a feature set for each team-season
    # and then creating matchups. This demonstrates the feature creation process.
    print("Data fetching complete.")
    return starters_df, bullpen_df, fielding_df, batting_df

# ==============================================================================
# 3. Feature Engineering Module (Upgraded)
# ==============================================================================
def create_advanced_features(starters, bullpen, fielding, batting):
    """Aggregates stats by team/season to create a comprehensive feature set."""
    print("Engineering advanced features...")

    # Aggregate starter stats by team and season
    starter_stats = starters.groupby(['Season', 'FG_Team']).agg(
        starter_siera=('SIERA', 'mean'),
        starter_whip=('WHIP', 'mean')
    ).reset_index()

    # Aggregate bullpen stats by team and season
    bullpen_stats = bullpen.groupby(['Season', 'FG_Team']).agg(
        bullpen_siera=('SIERA', 'mean'),
        bullpen_whip=('WHIP', 'mean')
    ).reset_index()

    # Align team abbreviations (FanGraphs vs. BRef) - this is a crucial, tricky step
    # For this script, we'll assume a mapping or use teams present in all datasets
    # A simple inner merge strategy will work for this example
    
    # Merge all stats into a single DataFrame per team per season
    team_features = pd.merge(batting, fielding, on=['Season', 'Tm'])
    # FanGraphs uses different abbreviations (e.g., 'WSN' vs 'WSH') so we need to be careful.
    # We will merge on Season and assume the team lists are mostly aligned.
    team_features = pd.merge(team_features, starter_stats, left_on=['Season', 'Tm'], right_on=['Season', 'FG_Team'], how='inner')
    team_features = pd.merge(team_features, bullpen_stats, left_on=['Season', 'Tm'], right_on=['Season', 'FG_Team'], how='inner')

    # Now, let's create a simulated set of games for training
    # In a real script, you'd merge this onto an actual game schedule
    from itertools import combinations
    
    all_matchups = []
    for season, group in team_features.groupby('Season'):
        for home_team, away_team in combinations(group.to_dict('records'), 2):
            # Add matchup in both home/away configurations
            all_matchups.append({**{'home_'+k: v for k,v in home_team.items()}, **{'away_'+k: v for k,v in away_team.items()}})
            all_matchups.append({**{'home_'+k: v for k,v in away_team.items()}, **{'away_'+k: v for k,v in home_team.items()}})

    matchups_df = pd.DataFrame(all_matchups)
    
    # Create differential features
    matchups_df['starter_siera_diff'] = matchups_df['away_starter_siera'] - matchups_df['home_starter_siera']
    matchups_df['starter_whip_diff'] = matchups_df['away_starter_whip'] - matchups_df['home_starter_whip']
    matchups_df['bullpen_siera_diff'] = matchups_df['away_bullpen_siera'] - matchups_df['home_bullpen_siera']
    matchups_df['bullpen_whip_diff'] = matchups_df['away_bullpen_whip'] - matchups_df['home_bullpen_whip']
    matchups_df['ops_plus_diff'] = matchups_df['home_OPS+'] - matchups_df['away_OPS+']
    matchups_df['drs_diff'] = matchups_df['home_DefEff'] - matchups_df['away_DefEff'] # Using DefEff as a proxy for DRS
    
    # --- Create a synthetic target variable for demonstration ---
    # A real model would use actual game outcomes. Here, we create a plausible target.
    # Home team is more likely to win if their pitchers are better (negative diffs) and offense is better (positive diff)
    score = -matchups_df['starter_siera_diff'] - matchups_df['bullpen_siera_diff'] + matchups_df['ops_plus_diff']
    win_prob = 1 / (1 + np.exp(-score * 0.5)) # Sigmoid function to get probability
    matchups_df[config.TARGET_COLUMN] = (np.random.rand(len(matchups_df)) < win_prob).astype(int)
    matchups_df['Season'] = matchups_df['home_Season'] # Get season for the split
    
    final_df = matchups_df.dropna(subset=config.FEATURE_COLUMNS + [config.TARGET_COLUMN]).reset_index(drop=True)
    print("Feature engineering complete.")
    return final_df

# ==============================================================================
# 4. Model Training and Evaluation (Upgraded)
# ==============================================================================
def train_and_evaluate_advanced():
    """Main pipeline for training, backtesting, and saving the advanced model."""
    
    # Step 1: Fetch and prepare data
    starters, bullpen, fielding, batting = fetch_advanced_historical_data(config.TRAIN_START_YEAR, config.TEST_YEAR)
    feature_df = create_advanced_features(starters, bullpen, fielding, batting)
    
    # Step 2: Temporal split
    train_df = feature_df[feature_df['Season'] <= config.TRAIN_END_YEAR].copy()
    test_df = feature_df[feature_df['Season'] == config.TEST_YEAR].copy()

    X_train = train_df[config.FEATURE_COLUMNS]
    y_train = train_df[config.TARGET_COLUMN]
    X_test = test_df[config.FEATURE_COLUMNS]
    y_test = test_df[config.TARGET_COLUMN]
    
    # Step 3: Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # --- Step 4: Implement Time-Decay Sample Weights ---
    print("\nCalculating time-decay sample weights...")
    # Give more weight to more recent seasons in the training data
    max_season = train_df['Season'].max()
    train_df['weight'] = np.exp((train_df['Season'] - max_season) * config.TIME_DECAY_RATE)
    sample_weight = train_df['weight'].values

    # --- Step 5: Train the XGBoost model ---
    print("Training advanced XGBoost model with sample weights...")
    model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        n_estimators=250,      # More estimators
        max_depth=4,           # Keep depth low to avoid overfitting
        learning_rate=0.05,    # Lower learning rate
        gamma=0.1,             # Regularization
        subsample=0.8,         # Row sampling
        colsample_bytree=0.8,  # Column sampling
        use_label_encoder=False,
        random_state=42
    )
    
    # *** For even better performance, uncomment and run this hyperparameter search ***
    # param_dist = {
    #     'n_estimators': [100, 250, 400],
    #     'learning_rate': [0.01, 0.05, 0.1],
    #     'max_depth': [3, 4, 5],
    #     'gamma': [0, 0.1, 0.2],
    #     'subsample': [0.7, 0.8, 0.9],
    #     'colsample_bytree': [0.7, 0.8, 0.9]
    # }
    # random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=25,
    #                                    scoring='roc_auc', cv=3, verbose=1, random_state=42)
    # random_search.fit(X_train_scaled, y_train, sample_weight=sample_weight)
    # model = random_search.best_estimator_ # Use the best model found
    # print("Best parameters found:", random_search.best_params_)
    
    model.fit(X_train_scaled, y_train, sample_weight=sample_weight)

    # --- Step 6: Backtest on the test year ---
    print(f"\n--- Backtest Results for {config.TEST_YEAR} Season ---")
    predictions = model.predict(X_test_scaled)
    probabilities = model.predict_proba(X_test_scaled)[:, 1]

    accuracy = accuracy_score(y_test, predictions)
    auc = roc_auc_score(y_test, probabilities)
    
    print(f"Accuracy: {accuracy:.4f}")
    print(f"AUC Score: {auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, predictions, target_names=['Away Win', 'Home Win']))

    # --- Step 7: Save the final artifacts ---
    print("\nSaving model and scaler to disk...")
    joblib.dump(model, config.MODEL_PATH)
    joblib.dump(scaler, config.SCALER_PATH)
    print(f"Model saved to: {config.MODEL_PATH}")
    print(f"Scaler saved to: {config.SCALER_PATH}")


if __name__ == '__main__':
    train_and_evaluate_advanced()

